# Techmeme News Scraper ğŸ•·ï¸ğŸ“°

Má»™t cÃ´ng cá»¥ Python Ä‘á»ƒ cÃ o dá»¯ liá»‡u tin tá»©c tá»« [Techmeme.com](https://www.techmeme.com/) - trang tá»•ng há»£p tin tá»©c cÃ´ng nghá»‡ hÃ ng Ä‘áº§u.

## âœ¨ TÃ­nh nÄƒng

- ğŸ¯ CÃ o dá»¯ liá»‡u tá»« trang chá»§ Techmeme
- ğŸ“„ TrÃ­ch xuáº¥t tiÃªu Ä‘á», liÃªn káº¿t, mÃ´ táº£ vÃ  thá»i gian
- ğŸ’¾ LÆ°u dá»¯ liá»‡u ra file JSON vÃ  CSV
- ğŸ”„ Tá»± Ä‘á»™ng retry khi cÃ³ lá»—i máº¡ng
- ğŸ“Š Logging chi tiáº¿t cho viá»‡c debug
- âš¡ Sá»­ dá»¥ng BeautifulSoup vÃ  requests

## ğŸ› ï¸ CÃ i Ä‘áº·t

### 1. Clone repository
```bash
git clone <repository-url>
cd Crawl-News
```

### 2. CÃ i Ä‘áº·t dependencies

#### Tá»± Ä‘á»™ng (Windows):
```bash
setup.bat
```

#### Tá»± Ä‘á»™ng (Linux/Mac):
```bash
chmod +x setup.sh
./setup.sh
```

#### Thá»§ cÃ´ng:
```bash
# Sá»­ dá»¥ng pip
pip install -r requirements.txt

# Hoáº·c sá»­ dá»¥ng uv (náº¿u cÃ³)
uv pip install -r requirements.txt

# Hoáº·c cÃ i Ä‘áº·t tá»«ng package
pip install beautifulsoup4 requests lxml
```

### 3. Kiá»ƒm tra cÃ i Ä‘áº·t
```bash
python -c "import requests, bs4, lxml; print('âœ… Setup completed!')"
```

## ğŸš€ Sá»­ dá»¥ng

### CÃ¡ch cÆ¡ báº£n
```bash
python techmeme.py
```

### Import vÃ o code khÃ¡c
```python
from techmeme import TechmemeScraper

# Khá»Ÿi táº¡o scraper
scraper = TechmemeScraper()

# CÃ o dá»¯ liá»‡u
articles = scraper.scrape_homepage()

# LÆ°u dá»¯ liá»‡u
scraper.save_to_json(articles, "my_articles.json")
scraper.save_to_csv(articles, "my_articles.csv")
```

## ğŸ“Š Dá»¯ liá»‡u Output

Má»—i bÃ i viáº¿t Ä‘Æ°á»£c trÃ­ch xuáº¥t bao gá»“m:

```json
{
  "title": "TiÃªu Ä‘á» bÃ i viáº¿t",
  "url": "https://example.com/article",
  "description": "MÃ´ táº£ ngáº¯n gá»n vá» bÃ i viáº¿t",
  "timestamp": "Thá»i gian Ä‘Äƒng",
  "source": "Nguá»“n tin",
  "scraped_at": "2025-07-06T10:30:00"
}
```

## ğŸ”§ Cáº¥u hÃ¬nh

### Thay Ä‘á»•i User-Agent
```python
scraper = TechmemeScraper()
scraper.session.headers.update({
    'User-Agent': 'Your Custom User Agent'
})
```

### Äiá»u chá»‰nh retry logic
```python
soup = scraper.get_page_content(url, retries=5)
```

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

```
Crawl-News/
â”œâ”€â”€ techmeme.py           # Script chÃ­nh
â”œâ”€â”€ requirements.txt      # Dependencies
â”œâ”€â”€ README.md            # Documentation
â”œâ”€â”€ .gitignore          # Git ignore rules
â””â”€â”€ logs/               # Log files (tá»± Ä‘á»™ng táº¡o)
```

## ğŸ› Troubleshooting

### Lá»—i thÆ°á»ng gáº·p

1. **KhÃ´ng cÃ o Ä‘Æ°á»£c dá»¯ liá»‡u**
   - Kiá»ƒm tra káº¿t ná»‘i internet
   - Website cÃ³ thá»ƒ Ä‘Ã£ thay Ä‘á»•i cáº¥u trÃºc HTML
   - Xem log file Ä‘á»ƒ debug

2. **403 Forbidden Error**
   - Thay Ä‘á»•i User-Agent
   - ThÃªm delay giá»¯a cÃ¡c request

3. **Timeout Error**
   - TÄƒng timeout trong requests
   - Kiá»ƒm tra tá»‘c Ä‘á»™ máº¡ng

### Debug
```bash
# Xem log chi tiáº¿t
tail -f techmeme_scraper.log

# Cháº¡y vá»›i debug level
python -c "import logging; logging.basicConfig(level=logging.DEBUG); exec(open('techmeme.py').read())"
```

## âš–ï¸ LÆ°u Ã½ phÃ¡p lÃ½

- TuÃ¢n thá»§ robots.txt cá»§a website
- KhÃ´ng spam request quÃ¡ nhiá»u
- Chá»‰ sá»­ dá»¥ng cho má»¥c Ä‘Ã­ch há»c táº­p/nghiÃªn cá»©u
- TÃ´n trá»ng báº£n quyá»n ná»™i dung

## ğŸ¤ ÄÃ³ng gÃ³p

1. Fork repository
2. Táº¡o feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push branch (`git push origin feature/amazing-feature`)
5. Táº¡o Pull Request

## ğŸ“ License

Dá»± Ã¡n nÃ y Ä‘Æ°á»£c phÃ¡t hÃ nh dÆ°á»›i MIT License. Xem file [LICENSE](LICENSE) Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t.

## ğŸ“ LiÃªn há»‡

Náº¿u cÃ³ cÃ¢u há»i hoáº·c gÃ³p Ã½, vui lÃ²ng táº¡o [Issue](../../issues) trÃªn GitHub.

---

**LÆ°u Ã½**: ÄÃ¢y lÃ  cÃ´ng cá»¥ há»c táº­p. Vui lÃ²ng sá»­ dá»¥ng má»™t cÃ¡ch cÃ³ trÃ¡ch nhiá»‡m vÃ  tuÃ¢n thá»§ cÃ¡c quy Ä‘á»‹nh vá» web scraping.