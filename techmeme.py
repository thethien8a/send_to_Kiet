#!/usr/bin/env python3
"""
Techmeme News Scraper

This script scrapes news articles from Techmeme.com - a tech news aggregator website.
It extracts article titles, links, descriptions, and timestamps.

Author: Generated by GitHub Copilot
Created: July 2025
"""

import requests
from bs4 import BeautifulSoup
import json
import csv
import time
import logging
from datetime import datetime
from typing import List, Dict, Optional
import os
import sys

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('techmeme_scraper.log'),
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)

class TechmemeScraper:
    """
    A scraper class for extracting news data from Techmeme.com
    """
    
    def __init__(self, base_url: str = "https://www.techmeme.com"):
        """
        Initialize the scraper
        
        Args:
            base_url: The base URL of Techmeme (default: https://www.techmeme.com)
        """
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36'
        })
        
    def get_page_content(self, url: str, retries: int = 3) -> Optional[BeautifulSoup]:
        """
        Fetch and parse a webpage
        
        Args:
            url: URL to fetch
            retries: Number of retry attempts
            
        Returns:
            BeautifulSoup object or None if failed
        """
        for attempt in range(retries):
            try:
                logger.info(f"Fetching {url} (attempt {attempt + 1})")
                response = self.session.get(url, timeout=10)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                logger.info(f"Successfully fetched and parsed {url}")
                return soup
                
            except requests.RequestException as e:
                logger.warning(f"Attempt {attempt + 1} failed for {url}: {e}")
                if attempt < retries - 1:
                    time.sleep(2 ** attempt)  # Exponential backoff
                else:
                    logger.error(f"All attempts failed for {url}")
                    
        return None
    
    def extract_articles(self, soup: BeautifulSoup) -> List[Dict]:
        """
        Extract article information from the parsed HTML
        
        Args:
            soup: BeautifulSoup object of the page
            
        Returns:
            List of dictionaries containing article data
        """
        articles = []
        
        try:
            # Find article containers (adjust selectors based on actual HTML structure)
            article_containers = soup.find_all('div', class_=['itc2'])
            
            logger.info(f"Found {len(article_containers)} potential article containers")
            
            for container in article_containers:
                try:
                    article_data = self._parse_article_container(container)
                    if article_data and article_data.get('title') and article_data.get('url'):
                        articles.append(article_data)
                except Exception as e:
                    logger.warning(f"Error parsing article container: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"Error extracting articles: {e}")
            
        logger.info(f"Successfully extracted {len(articles)} articles")
        return articles
    
    def _parse_article_container(self, container) -> Optional[Dict]:
        """
        Parse individual article container
        
        Args:
            container: BeautifulSoup element containing article info
            
        Returns:
            Dictionary with article data or None
        """
        article_data = {
            'url': '',
            'title': '',
            'date': '',
            'author': '',
            'content': '',
            'error': False
        }
        
        try:
            # Extract title and URL
            link = container.find('a',class_=["ourh"])
            if link:
                article_data['title'] = link.get_text(strip=True)
                article_data['url'] = link.get('href', '')
            
            # Make URL absolute if it's relative
            if article_data['url'] and article_data['url'].startswith('/'):
                article_data['url'] = self.base_url + article_data['url']
            
            # Extract description/summary
            desc_element = container.find('p') or container.find('div', class_=['description', 'summary', 'excerpt'])
            if desc_element:
                article_data['description'] = desc_element.get_text(strip=True)
            
            # Extract timestamp
            time_element = container.find('time') or container.find('span', class_=['time', 'date', 'timestamp'])
            if time_element:
                article_data['timestamp'] = time_element.get_text(strip=True)
            
            # Extract source
            source_element = container.find('span', class_=['source', 'author', 'site'])
            if source_element:
                article_data['source'] = source_element.get_text(strip=True)
                
            return article_data if article_data['title'] and article_data['url'] else None
            
        except Exception as e:
            logger.warning(f"Error parsing article container: {e}")
            return None
    
    def scrape_homepage(self) -> List[Dict]:
        """
        Scrape articles from Techmeme homepage
        
        Returns:
            List of article dictionaries
        """
        logger.info("Starting to scrape Techmeme homepage")
        soup = self.get_page_content(self.base_url)
        
        if not soup:
            logger.error("Failed to fetch homepage")
            return []
            
        articles = self.extract_articles(soup)
        logger.info(f"Scraped {len(articles)} articles from homepage")
        return articles
    
    def save_to_json(self, articles: List[Dict], filename: str = None) -> str:
        """
        Save articles to JSON file
        
        Args:
            articles: List of article dictionaries
            filename: Output filename (optional)
            
        Returns:
            Path to saved file
        """
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"techmeme_articles_{timestamp}.json"
            
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(articles, f, indent=2, ensure_ascii=False)
            logger.info(f"Saved {len(articles)} articles to {filename}")
            return filename
        except Exception as e:
            logger.error(f"Error saving to JSON: {e}")
            raise
    
    def save_to_csv(self, articles: List[Dict], filename: str = None) -> str:
        """
        Save articles to CSV file
        
        Args:
            articles: List of article dictionaries
            filename: Output filename (optional)
            
        Returns:
            Path to saved file
        """
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"techmeme_articles_{timestamp}.csv"
            
        if not articles:
            logger.warning("No articles to save")
            return filename
            
        try:
            fieldnames = articles[0].keys()
            with open(filename, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(articles)
            logger.info(f"Saved {len(articles)} articles to {filename}")
            return filename
        except Exception as e:
            logger.error(f"Error saving to CSV: {e}")
            raise


def main():
    """
    Main function to run the scraper
    """
    logger.info("Starting Techmeme scraper")
    
    try:
        scraper = TechmemeScraper()
        articles = scraper.scrape_homepage()
        
        if articles:
            # Save to both JSON and CSV
            json_file = scraper.save_to_json(articles)
            csv_file = scraper.save_to_csv(articles)
            
            print(f"\n‚úÖ Scraping completed successfully!")
            print(f"üìä Scraped {len(articles)} articles")
            print(f"üíæ Saved to:")
            print(f"   - JSON: {json_file}")
            print(f"   - CSV: {csv_file}")
            
            # Display first few articles as preview
            print(f"\nüì∞ Preview of first 3 articles:")
            for i, article in enumerate(articles[:3], 1):
                print(f"\n{i}. {article['title']}")
                print(f"   URL: {article['url']}")
                if article['description']:
                    print(f"   Description: {article['description'][:100]}...")
        else:
            print("‚ùå No articles found. The website structure might have changed.")
            logger.warning("No articles extracted")
            
    except KeyboardInterrupt:
        logger.info("Scraper interrupted by user")
        print("\n‚èπÔ∏è Scraper stopped by user")
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        print(f"‚ùå Error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()